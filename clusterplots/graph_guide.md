# Understanding the Experiment Graphs
## A Non-Expert's Guide to Linear MPC Performance

This guide explains the four charts generated by our cluster experiment. These charts prove that our distributed algorithm is **fast**, **efficient**, and **stable** when processing massive graphs on a supercomputer.

---

### **1. The "Speed" Graph (Convergence)**
**Filename:** `cluster_plot_1_1_degree_decay.png`

This graph answers the question: **"How quickly does the algorithm solve the problem?"**

* **What you are looking at:**
    * The **Blue Line** tracks the "Maximum Degree" (the complexity of the hardest part of the graph).
    * The **X-Axis** shows time steps (Phases).
    * The **Y-Axis** shows complexity (on a logarithmic scale).

* **What GOOD looks like:**
    * A **steep, straight line crashing downwards**.
    * This means the algorithm destroys the problem's complexity exponentially fast. It turns a massive problem into a trivial one in just 5 or 6 steps.

* **The Takeaway:**
    > The algorithm is **Exponentially Fast**. It doesn't chip away at the problem; it crushes it.

---

### **2. The "Efficiency" Graph (Linearity)**
**Filename:** `cluster_plot_2_1_linearity.png`

This graph answers the question: **"If we double the data size, does the memory cost explode?"**

* **What you are looking at:**
    * The **Blue Line** tracks the "Linear Factor" ($C$).
    * This factor measures how much memory each machine uses relative to the total input size.

* **What GOOD looks like:**
    * A **flat, horizontal line** (or a very low, stable curve).
    * This proves that the memory required per machine stays proportional ($O(N)$) even as the dataset grows huge.

* **The Takeaway:**
    > The algorithm is **Memory Efficient**. We can process 10x larger datasets without needing 100x more RAM. It adheres strictly to the "Linear Regime."

---

### **3. The "Stability" Graph (Robustness)**
**Filename:** `cluster_plot_robustness_density.png`

This graph answers the question: **"Does the algorithm break if the graph gets harder (denser)?"**

* **What you are looking at:**
    * The **Blue Line** tracks the number of **Rounds** needed to finish.
    * The **Red Dashed Line** tracks the **Time** in seconds.
    * The **X-Axis** shows "Density" (how many connections each node has).

* **What GOOD looks like:**
    * The **Blue Line** should be **flat** or a "step function" (rising very slowly). It should NOT shoot up diagonally.
    * The **Red Line** will naturally go up (doing more work takes more time), which is fine.

* **The Takeaway:**
    > The algorithm is **Robust**. Even if the graph becomes 10x more complex (denser), the algorithm doesn't get confused or stuck; it just takes a few more seconds to crunch the numbers.

---

### **4. The "Power" Graph (Strong Scaling)**
**Filename:** `cluster_plot_2_2_strong_scaling.png`

This graph answers the question: **"If we throw more computers at the problem, does it get faster?"**

* **What you are looking at:**
    * The **Green Line** tracks **Execution Time**.
    * The **X-Axis** shows the number of **CPU Cores** used (from 6 to 96).

* **What GOOD looks like:**
    * A **curve sloping downwards**.
    * As you move to the right (adding more CPUs), the time to finish should drop.
    * Eventually, the line will flatten out. This is normal (the "Speed Limit" of the network).

* **The Takeaway:**
    > The algorithm is **Scalable**. Adding more hardware successfully speeds up the computation, proving that we are effectively parallelizing the workload.

---

## **Summary Verification**

| Graph | Proof Provided | Success Indicator |
| :--- | :--- | :--- |
| **Decay** | Speed | Steep Drop |
| **Linearity** | Memory Safety | Flat Line |
| **Robustness** | Stability | Flat Rounds |
| **Scaling** | Parallelism | Downward Curve |